{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "UkI--4Adh3BV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import time\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import csv\n",
        "import urllib.request\n",
        "import multiprocessing as mp\n",
        "import numpy as np\n",
        "import tiktoken\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.c_proj(y)\n",
        "        return y"
      ],
      "metadata": {
        "id": "TsjptlH4oS3A"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu    = nn.GELU(approximate='tanh')\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "wYaO9-S6o0yy"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "eRFWuOvK9XqU"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50257\n",
        "    n_layer: int = 6\n",
        "    n_head: int = 8\n",
        "    n_embd: int = 512\n"
      ],
      "metadata": {
        "id": "XEDVkut59iTF"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # weight sharing scheme\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        # init params\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            std = 0.02\n",
        "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
        "                std *= (2 * self.config.n_layer) ** -0.5\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
        "        x = tok_emb + pos_emb\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, device_type):\n",
        "          # start with all of the candidate parameters (that require grad)\n",
        "          param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "          param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "\n",
        "\n",
        "          decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "          print(type(decay_params[0]))\n",
        "          nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "          optim_groups = [\n",
        "              {'params': decay_params, 'weight_decay': weight_decay},\n",
        "              {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "          ]\n",
        "\n",
        "          num_decay_params = sum(p.numel() for p in decay_params)\n",
        "\n",
        "          num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "          if master_process:\n",
        "              print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "              print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "          fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "          use_fused = fused_available and device_type == \"cuda\"\n",
        "          if master_process:\n",
        "              print(f\"using fused AdamW: {use_fused}\")\n",
        "          optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
        "          return optimizer\n"
      ],
      "metadata": {
        "id": "syYRNVHAQqfX"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tokenizer\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "eot = enc._special_tokens['<|endoftext|>']"
      ],
      "metadata": {
        "id": "-htZhoAH1lto"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Settings\n",
        "local_dir = \"twitter_finetune_shards\"\n",
        "shard_size = int(1e8)        # 100 million tokens per training shard\n",
        "val_shard_size = shard_size // 10  # 10 million tokens for validation shard (small val set)\n",
        "os.makedirs(local_dir, exist_ok=True)\n",
        "\n",
        "# Sentiment140 CSV URL (public dataset)\n",
        "csv_url = \"https://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\"\n",
        "zip_file = \"trainingandtestdata.zip\"\n",
        "csv_file = \"training.1600000.processed.noemoticon.csv\"\n",
        "\n",
        "# Download & unzip if not present\n",
        "if not os.path.exists(csv_file):\n",
        "    print(\"Downloading Sentiment140 dataset (81MB)...\")\n",
        "    urllib.request.urlretrieve(csv_url, zip_file)\n",
        "    import zipfile\n",
        "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "    os.remove(zip_file)\n",
        "else:\n",
        "    print(\"Dataset CSV found, skipping download.\")\n",
        "\n",
        "\n",
        "\n",
        "def tokenize(row):\n",
        "    text = row[-1]  # tweet text is last column in CSV\n",
        "    tokens = [eot] + enc.encode_ordinary(text)\n",
        "    return np.array(tokens, dtype=np.uint16)\n",
        "\n",
        "def write_datafile(filename, tokens_np):\n",
        "    np.save(filename, tokens_np)\n",
        "\n",
        "# Read CSV safely with utf-8 and error replace to avoid decode errors\n",
        "print(\"Reading CSV into memory...\")\n",
        "with open(csv_file, encoding='utf-8', errors='replace') as f:\n",
        "    reader = csv.reader(f)\n",
        "    rows = list(reader)\n",
        "\n",
        "print(f\"Total tweets: {len(rows)}\")\n",
        "\n",
        "# Multiprocessing pool for tokenizing tweets\n",
        "nprocs = max(1, os.cpu_count() // 2)\n",
        "with mp.Pool(nprocs) as pool:\n",
        "    shard_index = 0\n",
        "    is_val_shard = True\n",
        "    all_tokens_np = np.empty((val_shard_size,), dtype=np.uint16)\n",
        "    token_count = 0\n",
        "    progress_bar = None\n",
        "\n",
        "    for tokens in pool.imap(tokenize, rows, chunksize=64):\n",
        "        current_shard_size = val_shard_size if is_val_shard else shard_size\n",
        "\n",
        "        if token_count + len(tokens) < current_shard_size:\n",
        "            # Append tokens to current shard buffer\n",
        "            all_tokens_np[token_count:token_count+len(tokens)] = tokens\n",
        "            token_count += len(tokens)\n",
        "            # Setup progress bar on first write\n",
        "            if progress_bar is None:\n",
        "                progress_bar = tqdm(total=current_shard_size, unit=\"tokens\",\n",
        "                                    desc=f\"{'Val' if is_val_shard else 'Train'} Shard {shard_index}\")\n",
        "            progress_bar.update(len(tokens))\n",
        "        else:\n",
        "            # Fill remainder of current shard and save it\n",
        "            remainder = current_shard_size - token_count\n",
        "            all_tokens_np[token_count:token_count+remainder] = tokens[:remainder]\n",
        "            split = \"val\" if is_val_shard else \"train\"\n",
        "            filename = os.path.join(local_dir, f\"twitter_{split}_{shard_index:06d}.npy\")\n",
        "            write_datafile(filename, all_tokens_np)\n",
        "            shard_index += 1\n",
        "            if progress_bar:\n",
        "                progress_bar.close()\n",
        "                progress_bar = None\n",
        "            # leftover tokens start the next shard\n",
        "            leftover = tokens[remainder:]\n",
        "            if is_val_shard:\n",
        "                is_val_shard = False\n",
        "                all_tokens_np = np.empty((shard_size,), dtype=np.uint16)\n",
        "            all_tokens_np[:len(leftover)] = leftover\n",
        "            token_count = len(leftover)\n",
        "\n",
        "    # Write last partial shard if any tokens remain\n",
        "    if token_count != 0:\n",
        "        split = \"val\" if is_val_shard else \"train\"\n",
        "        filename = os.path.join(local_dir, f\"twitter_{split}_{shard_index:06d}.npy\")\n",
        "        write_datafile(filename, all_tokens_np[:token_count])\n",
        "        if progress_bar:\n",
        "            progress_bar.close()\n",
        "\n",
        "print(f\"✅ Done! Shards saved in '{local_dir}'\")\n"
      ],
      "metadata": {
        "id": "l2DirRFK-OC7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf3b4bd2-55fc-42cb-f0f2-7bdbbab4c3a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading Sentiment140 dataset (81MB)...\n",
            "Reading CSV into memory...\n",
            "Total tweets: 1600000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Val Shard 0: 100%|█████████▉| 9999981/10000000 [00:12<00:00, 778902.57tokens/s]\n",
            "Train Shard 1:  24%|██▍       | 24472289/100000000 [00:33<01:17, 969243.83tokens/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_tokens(filename):\n",
        "    npt = np.load(filename)\n",
        "    npt = npt.astype(np.int32)\n",
        "    ptt = torch.tensor(npt, dtype=torch.long)\n",
        "    return ptt\n"
      ],
      "metadata": {
        "id": "9qDo28yX-QI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoaderLite:\n",
        "    def __init__(self, B, T, process_rank, num_processes, split):\n",
        "        self.B = B\n",
        "        self.T = T\n",
        "        self.process_rank = process_rank\n",
        "        self.num_processes = num_processes\n",
        "        assert split in {'train', 'val'}\n",
        "\n",
        "        data_root = \"/content/twitter_finetune_shards\"\n",
        "        shards = os.listdir(data_root)\n",
        "        shards = [s for s in shards if split in s]\n",
        "        shards = sorted(shards)\n",
        "        shards = [os.path.join(data_root, s) for s in shards]\n",
        "        self.shards = shards\n",
        "        assert len(shards) > 0, f\"no shards found for split {split}\"\n",
        "        if master_process:\n",
        "            print(f\"found {len(shards)} shards for split {split}\")\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_shard = 0\n",
        "        self.tokens = load_tokens(self.shards[self.current_shard])\n",
        "        self.current_position = self.B * self.T * self.process_rank\n",
        "\n",
        "    def next_batch(self):\n",
        "        B, T = self.B, self.T\n",
        "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
        "        x = (buf[:-1]).view(B, T) # inputs\n",
        "        y = (buf[1:]).view(B, T) # targets\n",
        "        self.current_position += B * T * self.num_processes\n",
        "        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n",
        "            self.current_shard = (self.current_shard + 1) % len(self.shards)\n",
        "            self.tokens = load_tokens(self.shards[self.current_shard])\n",
        "            self.current_position = B * T * self.process_rank\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "ZATrHrWD-RjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NZy4Vdp6-V8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "import torch.distributed as dist\n"
      ],
      "metadata": {
        "id": "UdtEa0rQ-Xrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
        "if ddp:\n",
        "    assert torch.cuda.is_available(), \"for now i think we need CUDA for DDP\"\n",
        "    init_process_group(backend='nccl')\n",
        "    ddp_rank = int(os.environ['RANK'])\n",
        "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
        "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
        "    device = f'cuda:{ddp_local_rank}'\n",
        "    torch.cuda.set_device(device)\n",
        "    master_process = ddp_rank == 0  #this process will do logging, checkpointing etc.\n",
        "else:\n",
        "    #non-DDP run\n",
        "    ddp_rank = 0\n",
        "    ddp_local_rank = 0\n",
        "    ddp_world_size = 1\n",
        "    master_process = True\n",
        "\n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda\"\n",
        "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "        device = \"mps\"\n",
        "    print(f\"using device: {device}\")\n",
        "\n",
        "device_type = \"cuda\" if device.startswith(\"cuda\") else \"cpu\"\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(1337)\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "total_batch_size = 512*8\n",
        "B = 8\n",
        "T = 512\n",
        "assert total_batch_size % (B * T * ddp_world_size) == 0, \"make sure total_batch_size is divisible by B * T * ddp_world_size\"\n",
        "grad_accum_steps = total_batch_size // (B * T * ddp_world_size)\n",
        "if master_process:\n",
        "    print(f\"total desired batch size: {total_batch_size}\")\n",
        "    print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")\n",
        "\n",
        "train_loader = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, split=\"train\")\n",
        "val_loader = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, split=\"val\")\n",
        "\n",
        "torch.set_float32_matmul_precision('high')"
      ],
      "metadata": {
        "id": "HvuK1JTd-czr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT(GPTConfig(vocab_size=50304))\n",
        "model.to(device)\n",
        "use_compile = False\n",
        "if use_compile:\n",
        "    model = torch.compile(model)\n",
        "if ddp:\n",
        "    model = DDP(model, device_ids=[ddp_local_rank])\n",
        "raw_model = model.module if ddp else model\n",
        "\n",
        "\n",
        "max_lr = 6e-4\n",
        "min_lr = max_lr * 0.1\n",
        "warmup_steps = 715\n",
        "max_steps = 19073 #"
      ],
      "metadata": {
        "id": "zPKN7wlJ-mpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_steps:\n",
        "        return max_lr * (it+1) / warmup_steps\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > max_steps:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
        "    return min_lr + coeff * (max_lr - min_lr)"
      ],
      "metadata": {
        "id": "Kpq14OhC-pzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = raw_model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device_type=device_type)"
      ],
      "metadata": {
        "id": "ZUNTl_RZ-rTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "LW2kyeryXQ-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_dir = \"log\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "log_file = os.path.join(log_dir, f\"log.txt\")\n",
        "with open(log_file, \"w\") as f: # open for writing to clear the file\n",
        "    pass\n",
        "\n",
        "for step in range(max_steps):\n",
        "    t0 = time.time()\n",
        "    last_step = (step == max_steps - 1)\n",
        "\n",
        "    # once in a while evaluate our validation loss\n",
        "    if step % 250 == 0 or last_step:\n",
        "        model.eval()\n",
        "        val_loader.reset()\n",
        "        with torch.no_grad():\n",
        "            val_loss_accum = 0.0\n",
        "            val_loss_steps = 20\n",
        "            for _ in range(val_loss_steps):\n",
        "                x, y = val_loader.next_batch()\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
        "                    logits, loss = model(x, y)\n",
        "                loss = loss / val_loss_steps\n",
        "                val_loss_accum += loss.detach()\n",
        "        if ddp:\n",
        "            dist.all_reduce(val_loss_accum, op=dist.ReduceOp.AVG)\n",
        "        if master_process:\n",
        "            print(f\"validation loss: {val_loss_accum.item():.4f}\")\n",
        "            with open(log_file, \"a\") as f:\n",
        "                f.write(f\"{step} val {val_loss_accum.item():.4f}\\n\")\n",
        "            if step >= 0 and (step % 500 == 0 or last_step):\n",
        "                # optionally write model checkpoints\n",
        "                checkpoint_path = os.path.join('/content/drive/MyDrive/model.pt')\n",
        "                checkpoint = {\n",
        "                    'model': raw_model.state_dict(),\n",
        "                    'config': raw_model.config,\n",
        "                    'step': step,\n",
        "                    'val_loss': val_loss_accum.item()\n",
        "                }\n",
        "\n",
        "                torch.save(checkpoint, checkpoint_path)\n",
        "\n",
        "\n",
        "    # do one step of the optimization\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    loss_accum = 0.0\n",
        "    for micro_step in range(grad_accum_steps):\n",
        "        x, y = train_loader.next_batch()\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        if ddp:\n",
        "            model.require_backward_grad_sync = (micro_step == grad_accum_steps - 1)\n",
        "        with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
        "            logits, loss = model(x, y)\n",
        "        # we have to scale the loss to account for gradient accumulation,\n",
        "        # because the gradients just add on each successive backward().\n",
        "        # addition of gradients corresponds to a SUM in the objective, but\n",
        "        # instead of a SUM we want MEAN. Scale the loss here so it comes out right\n",
        "        loss = loss / grad_accum_steps\n",
        "        loss_accum += loss.detach()\n",
        "        loss.backward()\n",
        "    if ddp:\n",
        "        dist.all_reduce(loss_accum, op=dist.ReduceOp.AVG)\n",
        "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(step)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    optimizer.step()\n",
        "    if device_type == \"cuda\":\n",
        "        torch.cuda.synchronize() # wait for the GPU to finish work\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0 # time difference in seconds\n",
        "    tokens_processed = train_loader.B * train_loader.T * grad_accum_steps * ddp_world_size\n",
        "    tokens_per_sec = tokens_processed / dt\n",
        "    if master_process:\n",
        "        print(f\"step {step:5d} | loss: {loss_accum.item():.6f} | lr {lr:.4e} | norm: {norm:.4f} | dt: {dt*1000:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")\n",
        "        with open(log_file, \"a\") as f:\n",
        "            f.write(f\"{step} train {loss_accum.item():.6f}\\n\")\n",
        "\n",
        "if ddp:\n",
        "    destroy_process_group()"
      ],
      "metadata": {
        "id": "JUL-hHAr-smE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate(model, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "    \"\"\"\n",
        "    Generate new tokens iteratively until end of text or max length is reached.\n",
        "\n",
        "    Args:\n",
        "        model: The trained PyTorch model.\n",
        "        idx: Initial sequence of tokens (shape (B, T)).\n",
        "        max_new_tokens: Maximum number of tokens to generate.\n",
        "        temperature: Softmax temperature for sampling.\n",
        "        top_k: Consider only the top_k most likely tokens.\n",
        "\n",
        "    Returns:\n",
        "        Generated sequence of tokens (shape (B, T + max_new_tokens)).\n",
        "    \"\"\"\n",
        "    for _ in range(max_new_tokens):\n",
        "        # Crop context if longer than model block size\n",
        "        idx_cond = idx if idx.size(1) <= model.config.block_size else idx[:, -model.config.block_size:]\n",
        "        # Forward pass to get logits\n",
        "        logits, _ = model(idx_cond)\n",
        "        # Take logits for last token and scale by temperature\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "        # Optionally filter logits to top_k tokens only\n",
        "        if top_k is not None:\n",
        "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "        # Softmax to get probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        # Sample from distribution\n",
        "        idx_next = torch.multinomial(probs, num_samples=1)\n",
        "        # Append sampled token\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "        # Stop if EOS token sampled\n",
        "        if idx_next.item() == enc._special_tokens['<|endoftext|>']:\n",
        "            break\n",
        "    return idx\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ffd3boLgTWq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = GPT(GPTConfig(vocab_size=50304))\n"
      ],
      "metadata": {
        "id": "8l70njFnnLj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device='cuda'\n",
        "checkpoint_path = '/content/drive/MyDrive/model.pt'\n",
        "checkpoint = torch.load(checkpoint_path, weights_only=False)\n",
        "m = GPT(checkpoint['config'])\n",
        "m.load_state_dict(checkpoint['model'])\n",
        "m.to(device)\n",
        "print(f\"Model loaded from checkpoint saved at step {checkpoint['step']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdhHYbLZnbyR",
        "outputId": "dba183a0-575c-4ced-a364-aad942da0fc5"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded from checkpoint saved at step 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_token = torch.tensor([[enc._special_tokens['<|endoftext|>'],]], device='cuda', dtype=torch.long)\n",
        "\n",
        "generated_tokens = generate(\n",
        "    model=m,\n",
        "    idx=start_token,\n",
        "    max_new_tokens=100,\n",
        "    temperature=1.0,\n",
        "    top_k=50\n",
        ")\n",
        "\n",
        "print(\"Generated token IDs:\", generated_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wvPhHOw3jYE",
        "outputId": "28f945ce-13ec-4b11-adc1-269fb6295137"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated token IDs: tensor([[50256, 13025, 33796, 45987,  7660, 12039, 48189,  7257, 14425,  5093,\n",
            "         11555, 23209, 18802, 31419, 46849, 24778, 17119, 43870, 41993, 25259,\n",
            "         46761, 18380, 22090, 19634,  9131, 40115, 14922, 14922, 12434, 46351,\n",
            "         35763,  2387,  7670,  3491, 24878,  1700, 20780, 29899,  4355,  1601,\n",
            "         12434, 25960, 14074, 42423, 32591, 11156, 36263, 43628, 35339, 47625,\n",
            "         39863, 37538, 14922, 43048, 18542, 24532, 47982, 39744, 24651, 10839,\n",
            "         11286, 46220, 48227, 24276,  7026, 41749, 27129, 44486, 29674,   253,\n",
            "         25843, 30222,  3833, 36342, 44448, 31034, 12465, 46059, 10461, 42456,\n",
            "          1787, 29377, 28560, 39767, 28756,  1366, 48012, 31283, 31283, 12289,\n",
            "         12494, 47696, 30845, 24414, 33485,  4168, 30084, 43489,  8609, 31283,\n",
            "         39245]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(enc.decode(generated_tokens.tolist()[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUwfcgAv3pX1",
        "outputId": "7c710b3f-700a-4bab-adfc-011dbfbdb60f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|endoftext|> reasonablyImportant026 finger Must PSU CA denial north velmc ERAICLE felonMartin warriorsAMIdinand pitchers771Softribes glow LA HOiopiop Driver925 Hiroshiresvy star semester record bree Guysacityury Driver Cambod Ottawa unconditional 302 assemb unrestrictedencryptedpkgviation goodies Gilesiop Wouldn illustrated Eugeneowicz invoking sexist voicesagen goblins Occupations sting cheap Yusarious unilaterally Mohammad�ravel Taken pressure videogierrez (~ tremendous propell Evilhaust pot BRE 340 Chromebookoccupied data footprints boun boun mothersootingBloomberg BlackBerry 1939helm skin demographics445rix boun Paso\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aklKdaMz32fT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}